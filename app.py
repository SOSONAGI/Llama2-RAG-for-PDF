# Import streamlit for app dev
import streamlit as st

# Import transformer classes for generation
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
# Import torch for datatype attributes 
import torch
# Import the prompt wrapper...but for llama index
from llama_index.prompts.prompts import SimpleInputPrompt
# Import the llama index HF Wrapper
from llama_index.llms import HuggingFaceLLM
# Bring in embeddings wrapper
from llama_index.embeddings import LangchainEmbedding
# Bring in HF embeddings - need these to represent document chunks
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
# Bring in stuff to change service context
from llama_index import set_global_service_context
from llama_index import ServiceContext
# Import deps to load documents 
from llama_index import VectorStoreIndex, download_loader
from pathlib import Path
import tempfile

# Define variable to hold llama2 weights naming 
name = "model_name from huggingface or your own trained model_path"
# Set auth token variable from hugging face 
auth_token = "your huggingface auth_token(write)"

@st.cache_resource
def get_tokenizer_model():
    # Create tokenizer
    tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='./model/', use_auth_token=auth_token)

    # Create model
    model = AutoModelForCausalLM.from_pretrained(name, cache_dir='./model/'
                            , use_auth_token=auth_token, torch_dtype=torch.float16, 
                            rope_scaling={"type": "dynamic", "factor": 2}, load_in_8bit=True, device_map='auto') 

    return tokenizer, model
tokenizer, model = get_tokenizer_model()

# Create a system prompt 
system_prompt = """<s>[INST] <<SYS>>
ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ê³ , ì˜ˆì˜ë°”ë¥´ê³ , ì •ì§í•œ ë³´ì¡°ìì…ë‹ˆë‹¤. í•­ìƒ ê°€ëŠ¥í•œ í•œ ë„ì›€ì´ ë˜ë„ë¡ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”, ë‹¨ ì•ˆì „í•˜ê²Œ. ë‹¹ì‹ ì˜ ë‹µë³€ì—ëŠ” í•´ë¡œìš´, ë¹„ìœ¤ë¦¬ì ì¸, ì¸ì¢…ì°¨ë³„ì ì¸, ì„±ì°¨ë³„ì ì¸, ë…ì„± ìˆëŠ”, ìœ„í—˜í•œ ë˜ëŠ” ë¶ˆë²•ì ì¸ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
ì‘ë‹µì´ ì‚¬íšŒì ìœ¼ë¡œ í¸ê²¬ì´ ì—†ê³  ê¸ì •ì ì¸ ì„±ê²©ì„ ê°€ì§€ë„ë¡ í•´ ì£¼ì„¸ìš”.

ì§ˆë¬¸ì´ ì•„ë¬´ëŸ° ì˜ë¯¸ê°€ ì—†ê±°ë‚˜ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ì—†ë‹¤ë©´, ì™œ ê·¸ëŸ°ì§€ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”, ëŒ€ì‹  í‹€ë¦° ê²ƒì„ ëŒ€ë‹µí•˜ì§€ ë§ˆì„¸ìš”. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´, ê±°ì§“ ì •ë³´ë¥¼ ê³µìœ í•˜ì§€ ë§ˆì„¸ìš”.

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” PDF í˜•ì‹ì˜ ë¬¸ì„œì™€ ê´€ë ¨ëœ ëª¨ë“  ê²ƒì— ëŒ€í•œ ë‹µì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.<</SYS>>
"""
# Throw together the query wrapper
query_wrapper_prompt = SimpleInputPrompt("{query_str} [/INST]")

# Create a HF LLM using the llama index wrapper 
llm = HuggingFaceLLM(context_window=2048,
                    max_new_tokens=1024,
                    system_prompt=system_prompt,
                    query_wrapper_prompt=query_wrapper_prompt,
                    model=model,
                    tokenizer=tokenizer)

# Create and dl embeddings instance  
embeddings=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
)

# Create new service context instance
service_context = ServiceContext.from_defaults(
    chunk_size=1024,
    chunk_overlap=512,
    llm=llm,
    embed_model=embeddings
)
# And set the service context
set_global_service_context(service_context)

# Download PDF Loader 
PyMuPDFReader = download_loader("PyMuPDFReader")
# Create PDF Loader
loader = PyMuPDFReader()

# File uploader for the user to upload PDF file
uploaded_file = st.file_uploader("Choose a file", type=["pdf"])

if uploaded_file is not None:
    # Create a temporary file to store the uploaded file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(uploaded_file.read())
        temp_path = temp_file.name

    # Load documents 
    documents = loader.load(file_path=temp_path, metadata=True)

    # Create an index - we'll be able to query this in a sec
    index = VectorStoreIndex.from_documents(documents)
    # Setup index query engine using LLM 
    query_engine = index.as_query_engine()

    # Create centered main title 
    st.title('ğŸ¦™ Llama RAG for PDF')
    # Create a text input box for the user
    prompt = st.text_input('Input your prompt here')

    # If the user hits enter
    if prompt:
        response = query_engine.query(prompt)
        # ...and write it out to the screen
        st.write(response)

        # Display raw response object
        with st.expander('Response Object'):
            st.write(response)
        # Display source text
        with st.expander('Source Text'):
            st.write(response.get_formatted_sources())
